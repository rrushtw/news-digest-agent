import logging
import random
import time

from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

from .base import BaseScraper


class CteeScraper(BaseScraper):
    def fetch_new_articles(self, known_urls=None):
        if known_urls is None:
            known_urls = set()

        url = self.config.get("source_url")
        selectors = self.config.get("scraper_config", {})

        # ä½¿ç”¨ Playwright å•Ÿå‹•ç€è¦½å™¨
        with sync_playwright() as p:
            # å•Ÿå‹• Chromium (headless=True ä»£è¡¨ä¸é¡¯ç¤ºè¦–çª—ï¼Œé©åˆåœ¨ Docker/Server è·‘)
            # browser = p.chromium.launch(headless=True)
            browser = p.chromium.launch(
                headless=True,
                args=[
                    "--disable-blink-features=AutomationControlled",  # é—œéµï¼šéš±è—è‡ªå‹•åŒ–æ¨™è¨˜
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                ],
            )

            # è¨­å®š Context (å½è£æˆä¸€èˆ¬ç€è¦½å™¨)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                ignore_https_errors=True,
            )

            # æ³¨å…¥ JavaScriptï¼Œå¾¹åº•ç§»é™¤ webdriver å±¬æ€§
            context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)

            page = context.new_page()

            try:
                # --- 1. æŠ“å–åˆ—è¡¨é  ---
                logging.info(f"Fetching list from: {url} (using Playwright)")

                # å‰å¾€é é¢ï¼Œç­‰å¾…ç¶²è·¯é–’ç½® (ç¢ºä¿è¼‰å…¥å®Œæˆ)
                response = page.goto(url, wait_until="domcontentloaded", timeout=15000)

                if response.status != 200:
                    logging.error(f"Failed to fetch list. Status: {response.status}")
                    return []

                # éš¨æ©Ÿå»¶é²ä¸€ä¸‹ï¼Œæ¨¡æ“¬çœŸäºº
                time.sleep(random.uniform(1, 3))

                # å–å¾—æ¸²æŸ“å¾Œçš„ HTML çµ¦ BeautifulSoup è§£æ
                html = page.content()
                soup = BeautifulSoup(html, "lxml")

                target_sel = selectors.get("target_selector", "h3.news-title a")
                link_tags = soup.select(target_sel)[:20]  # åªçœ‹å‰ 20 ç¯‡

                if not link_tags:
                    logging.warning(
                        f"No article links found with selector: {target_sel}"
                    )
                    # æˆªåœ–é™¤éŒ¯ (è‹¥å¤±æ•—ï¼Œé€™å¼µåœ–æœƒå¹«åŠ©å¾ˆå¤§)
                    # page.screenshot(path="debug_list_failed.png")
                    return []

                results = []

                for tag in link_tags:
                    article_url = tag["href"]
                    if not article_url.startswith("http"):
                        article_url = "https://www.ctee.com.tw" + article_url

                    title = tag.get_text().strip()

                    # å¦‚æœæ¨™é¡ŒåŒ…å«ã€Œæˆ°ç¸¾ã€ï¼Œç›´æ¥ç•¥é
                    if "æˆ°ç¸¾" in title:
                        logging.info(
                            f"ğŸš« Ignoring performance report (image only): {title}"
                        )
                        continue

                    if article_url in known_urls:
                        logging.info(f"Found known article, stopping scan: {title}")
                        break

                    results.append({"url": article_url, "title": title})

                return results

            except Exception as e:
                logging.error(f"Error in CteeScraper (Playwright): {e}")
                return []
            finally:
                browser.close()
