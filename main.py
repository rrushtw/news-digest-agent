# main.py
import logging
import os
import time
from typing import Set

import yaml
from dotenv import load_dotenv

from src.notifiers.email_sender import EmailNotifier
from src.processors.gemini import GeminiProcessor
from src.scrapers.ctee import CteeScraper

# è¨­å®š Log æ ¼å¼
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# è¼‰å…¥ .env ç’°å¢ƒè®Šæ•¸
load_dotenv()

# æ­·å²ç´€éŒ„æª”æ¡ˆçš„è·¯å¾‘
HISTORY_FILE = "history.txt"


def load_config(config_path):
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def get_scraper(config):
    if "ctee.com.tw" in config.get("source_url", ""):
        return CteeScraper(config)
    return None


def load_history() -> Set[str]:
    if not os.path.exists(HISTORY_FILE):
        return set()
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())


def append_history(urls: list[str] | str):
    """
    - å„²å­˜å·²å¯„é€çš„ç´€éŒ„
    - åªä¿ç•™æœ€è¿‘ 50 ç­†ï¼Œé¿å…æª”æ¡ˆç„¡é™è†¨è„¹
    """
    if not urls:
        return

    sent_urls = []
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            sent_urls = f.read().splitlines()

    # åŠ å…¥æ–° URL
    if isinstance(urls, list):
        sent_urls.extend(urls)
    else:
        sent_urls.append(urls)

    # å¯«å›æª”æ¡ˆ (åªç•™æœ€å¾Œ 100 ç­†)
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(sent_urls[-100:]))


def process_single_config(filename, config_dir, processor, notifier):
    """
    è™•ç†å–®ä¸€è¨­å®šæª”çš„å®Œæ•´æµç¨‹
    """
    if not (filename.endswith(".yaml") or filename.endswith(".yml")):
        return

    logging.info(f"Processing config: {filename}")
    config_path = os.path.join(config_dir, filename)
    config = load_config(config_path)

    # 1. Scrape
    scraper = get_scraper(config)
    if not scraper:
        logging.warning(f"No scraper found for {config.get('name')}")
        return

    history_set = load_history()

    # æŠ“å–å¤šç¯‡æ–°æ–‡ç« 
    new_articles = scraper.fetch_new_articles(known_urls=history_set)

    if not new_articles:
        logging.info(f"[{config.get('name')}] No new articles found.")
        return

    logging.info(
        f"[{config.get('name')}] Found {len(new_articles)} new articles. Processing..."
    )

    # ç”¨ä¾†æ”¶é›†æ‰€æœ‰æ–‡ç« çš„ HTML ç‰‡æ®µ
    email_body_parts = []
    success_urls = []

    for i, article in enumerate(new_articles):
        # é™¤äº†ç¬¬ä¸€ç¯‡ï¼Œä¹‹å¾Œçš„æ¯ä¸€ç¯‡éƒ½è¦å…ˆä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…è§¸ç™¼ API Rate Limit (429)
        if i > 0:
            logging.info(
                "â³ Sleeping for 15 seconds to respect Gemini API rate limits..."
            )
            time.sleep(15)

        logging.info(f"Processing Article with AI: {article['title']}")

        prompt = config.get("ai_prompt")
        # friendly_html = processor.process(article["content"], prompt)
        friendly_html = processor.process_url(article["url"], article["title"], prompt)

        if not friendly_html:
            logging.warning(f"Skipping {article['title']} due to AI error.")
            continue

        # ç‚ºæ¯ä¸€ç¯‡æ–‡ç« åŠ ä¸Šæ¨™é¡Œå€å¡Šï¼Œæ–¹ä¾¿å€éš”
        article_block = f"""
        <div style="border: 1px solid #ccc; padding: 20px; margin-bottom: 30px; border-radius: 10px; background-color: #fff;">
            <h2 style="color: #d32f2f; border-bottom: 2px solid #d32f2f; padding-bottom: 10px;">ğŸ“° {article["title"]}</h2>
            {friendly_html}
            <p style="text-align: right;"><a href="{article["url"]}" style="color: #007bff;">é–±è®€åŸæ–‡</a></p>
        </div>
        """
        email_body_parts.append(article_block)
        success_urls.append(article["url"])

    # æ•´åˆå¯„é€é‚è¼¯
    if len(email_body_parts) == 0:
        logging.warning("No valid content generated after processing.")
        return

    # çµ„åˆæ‰€æœ‰æ–‡ç« å…§å®¹
    combined_html = "".join(email_body_parts)

    # åŠ ä¸Š Email æ•´é«”å¤–æ¡† (ä¾‹å¦‚åŠ ä¸Šæ—¥æœŸæ¨™é¡Œ)
    full_email_content = f"""
    <html>
    <body style="font-family: sans-serif; background-color: #f4f4f4; padding: 20px;">
        <h1 style="text-align: center; color: #333;">ã€{config["name"]}ã€‘æœ€æ–°å¿«è¨Šå½™æ•´</h1>
        <p style="text-align: center; color: #666;">å…±æ•´ç†äº† {len(email_body_parts)} ç¯‡æ–°æ–‡ç« </p>
        {combined_html}
        <div style="text-align: center; margin-top: 40px; color: #888; font-size: 12px;">
            <p>æœ¬éƒµä»¶ç”± AI è‡ªå‹•æ•´ç†ç™¼é€ï¼Œåƒ…ä¾›åƒè€ƒã€‚</p>
            <p>ç¥æ‚¨ æ“ä½œé †åˆ© èº«é«”å¥åº·</p>
        </div>
    </body>
    </html>
    """

    subject = f"ã€{config['name']}ã€‘ä»Šæ—¥é‡é»å¿«è¨Š ({len(email_body_parts)} ç¯‡)"

    try:
        notifier.send(subject, full_email_content)
        # å¯„é€æˆåŠŸå¾Œï¼Œä¸€æ¬¡å°‡æ‰€æœ‰ URL å¯«å…¥æ­·å²ç´€éŒ„
        append_history(success_urls)
        logging.info(
            f"âœ… Batch email sent successfully with {len(success_urls)} articles."
        )
    except Exception as e:
        logging.error(f"Failed to send batch email: {e}")


def main():
    config_dir = "configs"

    # åˆå§‹åŒ–å…±ç”¨å…ƒä»¶
    processor = GeminiProcessor()
    notifier = EmailNotifier()

    # æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_dir):
        logging.error(f"Config directory '{config_dir}' not found.")
        return

    # æƒæä¸¦åŸ·è¡Œ
    for filename in os.listdir(config_dir):
        process_single_config(filename, config_dir, processor, notifier)


if __name__ == "__main__":
    main()
